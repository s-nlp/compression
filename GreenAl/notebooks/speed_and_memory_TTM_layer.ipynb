{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: /notebook/GreenAl\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "# BASE_DIR will be like '/home/jovyan/DemoExample/'\n",
    "BASE_DIR = pathlib.Path().absolute().parents[0]\n",
    "print(f\"Working dir: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/notebook')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlib.Path().absolute().parents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebook/GreenAl'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(str(BASE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import src.ttm_linear.ttm_linear\n",
    "from src.ttm_linear.ttm import TTM, einsum_forward, by_hands_forward, with_self_checkpoint, forward_backward_module, full_matrix_backward, full_einsum_backward, super_full_einsum_backward\n",
    "#import ttm_linear.old.linear\n",
    "from old.linear import TTMLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = t.device(\"cuda:2\" if t.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CORES = 11\n",
    "# DIMS = [[2, 2]] * N_CORES\n",
    "DIMS = [(8, 8), (8, 8), (12, 12), (1, 3)]\n",
    "N_CORES = len(DIMS)\n",
    "RANK = 16\n",
    "BS = 16 * 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analitical_memory():\n",
    "    parameters = (\n",
    "        sum(dims[0] * dims[1] * RANK for dims in DIMS[:1] + DIMS[-1:]) + \n",
    "        sum(dims[0] * dims[1] * RANK**2 for dims in DIMS[1:-1]) \n",
    "    )\n",
    "    \n",
    "    dims = np.array(DIMS)\n",
    "    \n",
    "    forward_tensors = [BS * np.prod(dims[:, 0])]\n",
    "    for i in range(N_CORES - 1):\n",
    "        forward_tensors.append(BS * np.prod(dims[i + 1:, 0]) * np.prod(dims[:i + 1, 1]) * RANK)\n",
    "    forward_tensors.append(BS * np.prod(dims[:, 1]))\n",
    "\n",
    "    print(f'parameters: {parameters / 2**20}')\n",
    "    print(f'forward tensors: {sum(forward_tensors) / 2**20}')\n",
    "    for forward_tensor in forward_tensors:\n",
    "        print(f'\\t{forward_tensor / 2**20}')\n",
    "    print(f'total number of parameters: {(parameters + sum(forward_tensors)) / 2**20}')\n",
    "\n",
    "    \n",
    "def cuda_memory(offset: int = 0):\n",
    "    return (t.cuda.memory_allocated(DEVICE)) / 2**20\n",
    "    \n",
    "\n",
    "def test(forward_backward_module):\n",
    "    #print(f'memory before start: {cuda_memory()}')\n",
    "    ttm = TTM(DIMS, RANK, forward_backward_module).to(DEVICE)\n",
    "    parameters = cuda_memory()\n",
    "    print(f'memory after layer initializing: {cuda_memory()}')\n",
    "\n",
    "    x = t.randn(BS, ttm.dim_in, requires_grad=True, device=DEVICE)\n",
    "    print(f'size of input tensor: {cuda_memory() - parameters}')\n",
    "    y = ttm(x)\n",
    "    \n",
    "    print(f'total memory after forward: {cuda_memory() - parameters}')\n",
    "    \n",
    "    y.mean().backward()\n",
    "    \n",
    "    print(f'total memory after backward: {cuda_memory() - parameters}')\n",
    "    print()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare different einsum strategies in TTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: 0.0518035888671875\n",
      "forward tensors: 624.0\n",
      "\t12.0\n",
      "\t192.0\n",
      "\t192.0\n",
      "\t192.0\n",
      "\t36.0\n",
      "total number of parameters: 624.0518035888672\n",
      "FORWARD and BACKWARD modules:  einsum_forward None\n",
      "memory after layer initializing: 72.52685546875\n",
      "size of input tensor: 48.0\n",
      "total memory after forward: 1008.2763671875\n",
      "total memory after backward: 240.20751953125\n",
      "\n",
      "FORWARD and BACKWARD modules:  einsum_forward full_matrix_backward\n",
      "memory after layer initializing: 72.52685546875\n",
      "size of input tensor: 48.0\n",
      "total memory after forward: 192.0\n",
      "total memory after backward: 240.20751953125\n",
      "\n",
      "FORWARD and BACKWARD modules:  by_hands_forward None\n",
      "memory after layer initializing: 72.52685546875\n",
      "size of input tensor: 48.0\n",
      "total memory after forward: 2544.06689453125\n",
      "total memory after backward: 240.20751953125\n",
      "\n",
      "FORWARD and BACKWARD modules:  by_hands_forward full_matrix_backward\n",
      "memory after layer initializing: 72.52685546875\n",
      "size of input tensor: 48.0\n",
      "total memory after forward: 192.0\n",
      "total memory after backward: 240.20751953125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analitical_memory()\n",
    "\n",
    "for forward_module in [einsum_forward, by_hands_forward]:\n",
    "    for backward_module in [None, full_matrix_backward]:\n",
    "        print('FORWARD and BACKWARD modules: ', forward_module.__name__, None if backward_module is None else backward_module.__name__)\n",
    "\n",
    "        if backward_module:\n",
    "            module = forward_backward_module(forward_module, backward_module(forward_module))\n",
    "        else:\n",
    "            module = forward_module\n",
    "            \n",
    "        test(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comapare OLD and NEW versions of TTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTM NEW, rank 16\n",
      "memory after layer initializing: 342.8193359375\n",
      "size of input tensor: -19.32666015625\n",
      "total memory after forward: 44.49267578125\n",
      "total memory after backward: 47.7001953125\n",
      "TTM OLD, rank 16\n",
      "[(4, 4), (4, 4), (4, 4), (4, 4), (3, 12)]\n",
      "memory after layer initializing: 15.46533203125\n",
      "total memory after forward: 67.76611328125\n",
      "total memory after backward: 19.515625\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TTM NEW, rank 32\n",
      "memory after layer initializing: 19.515625\n",
      "size of input tensor: 3.40576171875\n",
      "total memory after forward: 12.92138671875\n",
      "total memory after backward: 16.7421875\n",
      "TTM OLD, rank 32\n",
      "[(8, 8), (8, 8), (12, 8), (1, 6)]\n",
      "memory after layer initializing: 17.275390625\n",
      "total memory after forward: 29.845703125\n",
      "total memory after backward: 20.9091796875\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TTM NEW, rank 64\n",
      "memory after layer initializing: 20.9091796875\n",
      "size of input tensor: 5.375\n",
      "total memory after forward: 17.2841796875\n",
      "total memory after backward: 24.30078125\n",
      "TTM OLD, rank 64\n",
      "[(8, 8), (8, 8), (12, 8), (1, 6)]\n",
      "memory after layer initializing: 25.55029296875\n",
      "total memory after forward: 38.69091796875\n",
      "total memory after backward: 31.0673828125\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TTM NEW, rank 128\n",
      "memory after layer initializing: 31.0673828125\n",
      "size of input tensor: 5.98388671875\n",
      "total memory after forward: 28.05126953125\n",
      "total memory after backward: 43.068359375\n",
      "TTM OLD, rank 128\n",
      "[(16, 16), (16, 16), (3, 12)]\n",
      "memory after layer initializing: 54.1767578125\n",
      "total memory after forward: 69.6767578125\n",
      "total memory after backward: 72.3193359375\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for RANK in [16, 32, 64, 128]:\n",
    "    print (\"TTM NEW, rank\", RANK)\n",
    "    parameters = cuda_memory()\n",
    "    print(f'memory after layer initializing: {cuda_memory()}')\n",
    "    \n",
    "    ttm = TTM(DIMS, RANK, forward_backward_module(einsum_forward, full_matrix_backward(einsum_forward))).to(DEVICE)\n",
    "\n",
    "    x = t.randn(BS, ttm.dim_in, requires_grad=True, device=DEVICE)\n",
    "    print(f'size of input tensor: {cuda_memory() - parameters}')\n",
    "    y = ttm(x)\n",
    "    \n",
    "    print(f'total memory after forward: {cuda_memory()}')\n",
    "    \n",
    "    y.mean().backward()\n",
    "    \n",
    "    print(f'total memory after backward: {cuda_memory()}')\n",
    "    \n",
    "    print (\"TTM OLD, rank\", RANK)\n",
    "    \n",
    "    \n",
    "    ttm_old = TTMLinear(768, 768 * 4, RANK).to(DEVICE)\n",
    "    parameters = cuda_memory()\n",
    "    print(f'memory after layer initializing: {cuda_memory()}')\n",
    "\n",
    "    x = t.randn(BS, ttm.dim_in, requires_grad=True, device=DEVICE)\n",
    "    #print(f'size of input tensor: {cuda_memory() - parameters}')\n",
    "    y = ttm_old(x)\n",
    "    \n",
    "    print(f'total memory after forward: {cuda_memory()}')\n",
    "    \n",
    "    y.mean().backward()\n",
    "    \n",
    "    print(f'total memory after backward: {cuda_memory()}')\n",
    "    print(\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "einsum\n",
      "1.16 ms ± 372 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "18.3 ms ± 11.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "self-checkpoint einsum\n",
      "7.63 ms ± 1.05 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "25.5 ms ± 973 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "full matrix einsum\n",
      "7.65 ms ± 1.27 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "18.5 ms ± 3.18 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "full einsum einsum\n",
      "7.63 ms ± 447 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "45.7 ms ± 1.26 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "super full einsum einsum\n",
      "7.65 ms ± 1.18 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "42.5 ms ± 1.52 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for module_name, module in {\n",
    "    'einsum': einsum_forward,\n",
    "    'self-checkpoint einsum': with_self_checkpoint(einsum_forward),\n",
    "    'full matrix einsum': forward_backward_module(einsum_forward, full_matrix_backward(einsum_forward)),\n",
    "    'full einsum einsum': forward_backward_module(einsum_forward, full_einsum_backward(einsum_forward)),\n",
    "    'super full einsum einsum': forward_backward_module(einsum_forward, super_full_einsum_backward),\n",
    "}.items():\n",
    "    print(module_name)\n",
    "\n",
    "    ttm = TTM(DIMS, RANK, module).to(device=DEVICE)\n",
    "    x = t.randn(BS, ttm.dim_in, requires_grad=True, device=DEVICE)\n",
    "\n",
    "    def forward():\n",
    "        (ttm(x)**2).mean()\n",
    "        t.cuda.synchronize()\n",
    "\n",
    "    def forward_backward():\n",
    "        (ttm(x)**2).mean().backward()\n",
    "        t.cuda.synchronize()\n",
    "\n",
    "    %timeit forward()\n",
    "    %timeit forward_backward()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

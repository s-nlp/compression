{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Wikitext for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  181M  100  181M    0     0  5597k      0  0:00:33  0:00:33 --:--:-- 6633k\n"
     ]
    }
   ],
   "source": [
    "!curl https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip -o wikitext-103-v1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip wikitext-103-v1.zip\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 10:18:04.949897: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "working_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.8.10/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#dataset_train = TextDataset(tokenizer=tokenizer, \n",
    "                                #file_path=\"/notebook/greenAI/wikitext-103/wiki.train.tokens\", \n",
    "                                #block_size=512)\n",
    "\n",
    "dataset_test = TextDataset(tokenizer=tokenizer, \n",
    "                                file_path=\"./wikitext-103/wiki.valid.tokens\", \n",
    "                                block_size=512)\n",
    "\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.per_gpu_train_batch_size = 8\n",
    "args.per_gpu_eval_batch_size = 2\n",
    "args.n_gpu = 1\n",
    "args.num_train_epochs = 25\n",
    "args.seed = 42\n",
    "args.mlm = False\n",
    "args.device = device\n",
    "args.output_dir = working_dir \n",
    "args.fp16 = False\n",
    "args.max_grad_norm = 1.0\n",
    "args.logging_steps = 500.0\n",
    "args.eval_batch_size = 6\n",
    "args.save_total_limit = 2\n",
    "args.is_factorized = True\n",
    "args.local_rank = -1\n",
    "args.max_steps = -1\n",
    "args.per_gpu_train_batch_size = 8\n",
    "args.per_gpu_eval_batch_size = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model chekpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ttm_checkpoints'...\n",
      "remote: Enumerating objects: 25, done.\u001b[K\n",
      "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
      "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
      "remote: Total 25 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (25/25), done.\n",
      "Filtering content: 100% (5/5), 1.75 GiB | 41.97 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch master https://sayankotor:0GznmGfhjkm@gitlab.appliedai.tech/greenai/data/stage/ttm_checkpoints.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from help_trainer import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Config, GPT2LMHeadModel\n",
    "from greenAI_gpt.src.classes.gpt2_tt import GPT2_TT_Model\n",
    "# Initializing a GPT2 configuration\n",
    "configuration = GPT2Config()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].mlp.c_proj.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100% 244/244 [00:09<00:00, 25.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': tensor(17.5525), 'loss': 2.865195578727566}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing a model from the configuration\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "import torch\n",
    "model.load_state_dict(torch.load(working_dir + '/ttm_checkpoints/rank_0/checkpoint_best/model_tt.pth', map_location= torch.device(device)))\n",
    "\n",
    "evaluate(args, model.to(device), dataset_test, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenAI_gpt.src.classes.gpt2_tt import GPT2_TT_Model\n",
    "model = GPT2_TT_Model(configuration, rank = 32, bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model.load_state_dict(torch.load(working_dir + '/ttm_checkpoints/new_tt32/checkpoint-145000/model_tt.pth', map_location= torch.device(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100% 244/244 [00:13<00:00, 18.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': tensor(21.0633), 'loss': 3.0475341799806377}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(args, model.to(device), dataset_test, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenAI_gpt.src.classes.gpt2_tt import GPT2_TT_Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2_TT_Model(configuration, rank = 64, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100% 244/244 [00:12<00:00, 18.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': tensor(18.8167), 'loss': 2.934744954109192}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model.load_state_dict(torch.load(working_dir + '/ttm_checkpoints/new_tt64/checkpoint-145000/model_tt.pth', map_location= torch.device(device)))\n",
    "\n",
    "evaluate(args, model.to(device), dataset_test, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from greenAI_gpt.src.classes.gpt2_tt import GPT2_TT_Model\n",
    "\n",
    "model = GPT2_TT_Model(configuration, rank = 16, bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100% 244/244 [00:11<00:00, 21.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': tensor(21.6136), 'loss': 3.0733221349168995}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(working_dir + '/ttm_checkpoints/new_tt16/checkpoint-145000/model_tt.pth', map_location= torch.device(device)))\n",
    "\n",
    "evaluate(args, model.to(device), dataset_test, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rank 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from greenAI_gpt.src.classes.gpt2_tt import GPT2_TT_Model\n",
    "\n",
    "model = GPT2_TT_Model(configuration, rank = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100% 244/244 [00:12<00:00, 18.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': tensor(18.3227), 'loss': 2.9081405020150983}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model.load_state_dict(torch.load(working_dir + '/ttm_checkpoints/new_tt80/checkpoint-145000/model_tt.pth', map_location= torch.device(device)))\n",
    "\n",
    "evaluate(args, model.to(device), dataset_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107698944\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

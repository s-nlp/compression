{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.russian_superglue_models import SpanClassificationModel\n",
    "from transformers import AutoModel, PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_configs import (\n",
    "    TASK_NUM_CLASSES,\n",
    "    TASK_TO_CONFIG,\n",
    "    TASK_TO_NAME,\n",
    "    TASK_TYPES,\n",
    ")\n",
    "from utils.russian_superglue_models import (\n",
    "    BertForEntityChoice,\n",
    "    RobertaForEntityChoice,\n",
    "    SpanClassificationModel,\n",
    "    EntityChoiceModel\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "from typing import Dict\n",
    "from transformers import BertConfig, RobertaConfig, AutoModelForSequenceClassification, BertTokenizer, RobertaTokenizer\n",
    "\n",
    "@dataclass\n",
    "class ModelData:\n",
    "    config: object\n",
    "    tokenizer: object\n",
    "    task_types: Dict[str, object]\n",
    "\n",
    "\n",
    "MODEL_CLASSES: Dict[str, ModelData] = {\n",
    "    \"bert\": ModelData(\n",
    "        config=BertConfig,\n",
    "        tokenizer=BertTokenizer,\n",
    "        task_types={\n",
    "            \"classification\": BertForSequenceClassification,\n",
    "            \"entity_choice\": BertForEntityChoice,\n",
    "            \"span_classification\": SpanClassificationModel,\n",
    "        },\n",
    "    ),\n",
    "    \"roberta\": ModelData(\n",
    "        config=RobertaConfig,\n",
    "        tokenizer=RobertaTokenizer,\n",
    "        task_types={\n",
    "            \"classification\": RobertaForSequenceClassification,\n",
    "            \"entity_choice\": RobertaForEntityChoice,\n",
    "            \"span_classification\": SpanClassificationModel,\n",
    "        },\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args) -> PreTrainedModel:\n",
    "    \"\"\"\n",
    "    Returns a pre-trained model for a given task.\n",
    "\n",
    "    Args:\n",
    "        args: An object that contains the following fields:\n",
    "            - model_name: A string that represents the name of the pre-trained model.\n",
    "            - task_name: A string that represents the name of the task.\n",
    "\n",
    "    Returns:\n",
    "        An instance of a pre-trained model for the given task.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the model or task name is not found in the dictionaries.\n",
    "    \"\"\"\n",
    "    model_data = MODEL_CLASSES.get(args.model_name)\n",
    "    if not model_data:\n",
    "        raise ValueError(f\"Unknown model name: {args.model_name}\")\n",
    "    model_type = model_data.task_types.get(TASK_TYPES[args.task_name])\n",
    "    if not model_type:\n",
    "        raise ValueError(f\"Unknown task name: {args.task_name}\")\n",
    "    num_classes = TASK_NUM_CLASSES.get(args.task_name, 2)\n",
    "    if TASK_TYPES[args.task_name] == 'span_classification':\n",
    "        return SpanClassificationModel(\n",
    "            backbone=AutoModel.from_pretrained(args.model_name_or_path),\n",
    "            num_labels=num_classes,\n",
    "        )\n",
    "    elif TASK_TYPES[args.task_name] == 'entity_choice':\n",
    "        return EntityChoiceModel(\n",
    "            backbone=AutoModel.from_pretrained(args.model_name_or_path)\n",
    "        )\n",
    "    else:\n",
    "        return AutoModelForSequenceClassification.from_pretrained(\n",
    "            args.model_name_or_path, num_labels=num_classes\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, task_name, model_name, model_name_or_path):\n",
    "        self.task_name = task_name\n",
    "        self.model_name = model_name\n",
    "        self.model_name_or_path = model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EntityChoiceModel.__init__() got an unexpected keyword argument 'num_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m TASK_TO_NAME\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m      2\u001b[0m     args \u001b[39m=\u001b[39m Args(task, \u001b[39m'\u001b[39m\u001b[39mbert\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     get_model(args)\n",
      "Cell \u001b[0;32mIn [15], line 29\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m SpanClassificationModel(\n\u001b[1;32m     25\u001b[0m         backbone\u001b[39m=\u001b[39mAutoModel\u001b[39m.\u001b[39mfrom_pretrained(args\u001b[39m.\u001b[39mmodel_name_or_path),\n\u001b[1;32m     26\u001b[0m         num_labels\u001b[39m=\u001b[39mnum_classes,\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     28\u001b[0m \u001b[39melif\u001b[39;00m TASK_TYPES[args\u001b[39m.\u001b[39mtask_name] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mentity_choice\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mreturn\u001b[39;00m EntityChoiceModel(\n\u001b[1;32m     30\u001b[0m         backbone\u001b[39m=\u001b[39;49mAutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(args\u001b[39m.\u001b[39;49mmodel_name_or_path),\n\u001b[1;32m     31\u001b[0m         num_labels\u001b[39m=\u001b[39;49mnum_classes,\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     33\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m AutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     35\u001b[0m         args\u001b[39m.\u001b[39mmodel_name_or_path, num_labels\u001b[39m=\u001b[39mnum_classes\n\u001b[1;32m     36\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: EntityChoiceModel.__init__() got an unexpected keyword argument 'num_labels'"
     ]
    }
   ],
   "source": [
    "for task in TASK_TO_NAME.keys():\n",
    "    args = Args(task, 'bert', 'bert-base-uncased')\n",
    "    get_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args('russe', 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpanClassificationModel(\n",
       "  (backbone): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls_fc_layer): FCLayer(\n",
       "    (layers): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (e1_fc_layer): FCLayer(\n",
       "    (layers): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (e2_fc_layer): FCLayer(\n",
       "    (layers): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (label_classifier): FCLayer(\n",
       "    (layers): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(in_features=2304, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "from utils.dataset_configs import TASK_TO_NAME, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6737211bbf7a4738\n",
      "/mnt/storage/moskovskiy/workspace/anaconda3/envs/tc/lib/python3.10/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'storage.googleapis.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/moskovskiy/.cache/huggingface/datasets/json/default-6737211bbf7a4738/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa329bd62f6403b9d33bf2fa666a1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcff02ce308a4023ae60561a9ec12e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f98d0db04f7434bbe95ba42a8342338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0e43f68e0446fee4\n",
      "Found cached dataset json (/home/moskovskiy/.cache/huggingface/datasets/json/default-0e43f68e0446fee4/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/moskovskiy/.cache/huggingface/datasets/json/default-6737211bbf7a4738/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data('rwsd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_configs import TASK_TO_CONFIG\n",
    "config = TASK_TO_CONFIG['muserc'](dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7a0d62c7bc4d17b45f5b257eb96a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f76fa84ddf402d83658e4e7931dfbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/moskovskiy/.local/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/moskovskiy/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 585, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"/home/moskovskiy/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 552, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"/home/moskovskiy/.local/lib/python3.10/site-packages/datasets/fingerprint.py\", line 480, in wrapper\n    out = func(self, *args, **kwargs)\n  File \"/home/moskovskiy/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2982, in _map_single\n    batch = apply_function_on_filtered_inputs(\n  File \"/home/moskovskiy/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2865, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"/home/moskovskiy/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2545, in decorated\n    result = f(decorated_item, *args, **kwargs)\n  File \"/mnt/storage/moskovskiy/workspace/nlp/russian_superglue/utils/dataset_configs.py\", line 494, in process_data\n    for question, answer in zip(examples[\"question\"], examples[\"answer\"])\n  File \"/home/moskovskiy/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 149, in __getitem__\n    values = super().__getitem__(key)\n  File \"/mnt/storage/moskovskiy/workspace/anaconda3/envs/tc/lib/python3.10/collections/__init__.py\", line 1106, in __getitem__\n    raise KeyError(key)\nKeyError: 'question'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m partial\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizerFast\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mcointegrated/rubert-tiny2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m processed_dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m      5\u001b[0m     partial(\n\u001b[1;32m      6\u001b[0m         config\u001b[39m.\u001b[39;49mprocess_data, tokenizer\u001b[39m=\u001b[39;49mtokenizer, max_length\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m\n\u001b[1;32m      7\u001b[0m     ),\n\u001b[1;32m      8\u001b[0m     num_proc\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     keep_in_memory\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     10\u001b[0m     batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/dataset_dict.py:777\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    775\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    776\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 777\u001b[0m     {\n\u001b[1;32m    778\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    779\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[1;32m    780\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[1;32m    781\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[1;32m    782\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[1;32m    783\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[1;32m    784\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    785\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    786\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[1;32m    787\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    788\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    789\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    790\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    791\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[1;32m    792\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[1;32m    793\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[1;32m    794\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[1;32m    795\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[1;32m    796\u001b[0m         )\n\u001b[1;32m    797\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    798\u001b[0m     }\n\u001b[1;32m    799\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/dataset_dict.py:778\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    775\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    776\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    777\u001b[0m     {\n\u001b[0;32m--> 778\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    779\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m    780\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m    781\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m    782\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m    783\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m    784\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    785\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m    786\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m    787\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m    788\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    789\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[1;32m    790\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m    791\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    792\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m    793\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m    794\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    795\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    796\u001b[0m         )\n\u001b[1;32m    797\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    798\u001b[0m     }\n\u001b[1;32m    799\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2698\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[39massert\u001b[39;00m (\n\u001b[1;32m   2694\u001b[0m             \u001b[39mlen\u001b[39m(results) \u001b[39m==\u001b[39m nb_of_missing_shards\n\u001b[1;32m   2695\u001b[0m         ), \u001b[39m\"\u001b[39m\u001b[39mThe number of missing cached shards needs to correspond to the number of `_map_single` we\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre running\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2697\u001b[0m         \u001b[39mfor\u001b[39;00m index, async_result \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mitems():\n\u001b[0;32m-> 2698\u001b[0m             transformed_shards[index] \u001b[39m=\u001b[39m async_result\u001b[39m.\u001b[39;49mget()\n\u001b[1;32m   2700\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m   2701\u001b[0m     transformed_shards\u001b[39m.\u001b[39mcount(\u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   2702\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mAll shards have to be defined Datasets, none should still be missing.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2704\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConcatenating \u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m shards\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/multiprocess/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[0;31mKeyError\u001b[0m: 'question'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from functools import partial\n",
    "tokenizer = BertTokenizerFast.from_pretrained('cointegrated/rubert-tiny2')\n",
    "processed_dataset = dataset.map(\n",
    "    partial(\n",
    "        config.process_data, tokenizer=tokenizer, max_length=512\n",
    "    ),\n",
    "    num_proc=32,\n",
    "    keep_in_memory=True,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245\n"
     ]
    }
   ],
   "source": [
    "print(max([len(x[\"text\"]) for x in dataset[\"train\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'target': {'span1_text': '  ',\n",
       "  'span2_text': ' ',\n",
       "  'span1_index': 0,\n",
       "  'span2_index': 10},\n",
       " 'label': True,\n",
       " 'text': '       ,     .'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_configs import DatasetConfig\n",
    "from transformers import PreTrainedTokenizer, EvalPrediction\n",
    "from typing import List \n",
    "\n",
    "def find_sub_list(sublist: List[int], main_list: List[int]):\n",
    "    sublist_length=len(sublist)\n",
    "    for idx in (index for index, element in enumerate(main_list) if element == sublist[0]):\n",
    "        if main_list[idx: idx + sublist_length] == sublist:\n",
    "            start = idx\n",
    "            end = idx + sublist_length \n",
    "    print(start, end)\n",
    "    return start, end  \n",
    "\n",
    "class RWSDConfig(DatasetConfig):\n",
    "\n",
    "    best_metric: str = \"accuracy\"\n",
    "    num_classes: int = 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_data(examples, tokenizer: PreTrainedTokenizer, max_length: int):\n",
    "        # print(examples)\n",
    "        result = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=\"longest_first\",\n",
    "            return_token_type_ids=True,\n",
    "            max_length=245,\n",
    "        )\n",
    "        # print(result)\n",
    "\n",
    "        e1_masks, e2_masks = [], [] \n",
    "        \n",
    "        for i, sample in enumerate(examples[\"target\"]):\n",
    "            e1_mask = np.zeros_like(result[\"input_ids\"], dtype=int)\n",
    "            e2_mask = np.zeros_like(result[\"input_ids\"], dtype=int)\n",
    "\n",
    "            e1_span = tokenizer(\n",
    "                sample[\"span1_text\"],\n",
    "                add_special_tokens=False,\n",
    "                return_attention_mask=False,\n",
    "                return_token_type_ids=False,\n",
    "            )[\"input_ids\"]\n",
    "            e2_span = tokenizer(\n",
    "                sample[\"span2_text\"],\n",
    "                add_special_tokens=False,\n",
    "                return_attention_mask=False,\n",
    "                return_token_type_ids=False,\n",
    "            )[\"input_ids\"]\n",
    "            for mask, span in zip((e1_mask, e2_mask), (e1_span, e2_span)):\n",
    "                start, end = find_sub_list(span, result[\"input_ids\"][i])\n",
    "                mask[start: end] = 1\n",
    "\n",
    "            e1_masks.append(e1_mask)\n",
    "            e2_masks.append(e2_mask)\n",
    "\n",
    "        result[\"e1_mask\"] = e1_masks\n",
    "        result[\"e2_mask\"] = e2_masks\n",
    "\n",
    "        if isinstance(examples[\"label\"], list):\n",
    "            result[\"labels\"] = [int(x) for x in examples[\"label\"]]\n",
    "        else:\n",
    "            result[\"labels\"] = int(examples[\"label\"])\n",
    "        \n",
    "        return result  \n",
    "\n",
    "    def compute_metrics(self, predictions: EvalPrediction, split: str, **kwargs):\n",
    "        preds = np.argmax(predictions.predictions, axis=1)\n",
    "        return {\n",
    "            \"accuracy\": accuracy_score(\n",
    "                y_true=predictions.label_ids.astype(np.float32), y_pred=preds\n",
    "            )\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RWSDConfig(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizerFast\n",
    "# from functools import partial\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('cointegrated/rubert-tiny2')\n",
    "# processed_dataset = dataset.map(\n",
    "#     partial(\n",
    "#         config.process_data, tokenizer=tokenizer, max_length=255\n",
    "#     ),\n",
    "#     num_proc=32,\n",
    "#     keep_in_memory=True,\n",
    "#     batched=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "12 15)\n",
      "5 7, 15)\n",
      "12 15)\n",
      "1 4, 15)\n",
      "12 15)\n",
      "5 7, 15)\n",
      "12 15)\n",
      "1 2, 15)\n",
      "10 13)\n",
      "6 7, 13)\n",
      "10 13)\n",
      "1 2, 13)\n",
      "10 13)\n",
      "6 7, 13)\n",
      "10 13)\n",
      "1 2, 13)\n",
      "9 112)\n",
      "3 5 11)\n",
      "9 115)\n",
      "1 2 11)\n",
      "9 112)\n",
      "3 5 11)\n",
      "9 115)\n",
      "1 2 11)\n",
      "9 132)\n",
      "3 5 13)\n",
      "9 135)\n",
      "1 2 13)\n",
      "9 122)\n",
      "3 5 12)\n",
      "9 125)\n",
      "1 2 12)\n",
      "8 102)\n",
      "3 5 10)\n",
      "8 105)\n",
      "1 2 10)\n",
      "9 112)\n",
      "3 5 11)\n",
      "9 115)\n",
      "1 4 11)\n",
      "10 12)\n",
      "6 7, 12)\n",
      "10 12)\n",
      "1 4, 12)\n",
      "10 12)\n",
      "6 7, 12)\n",
      "10 12)\n",
      "1 2, 12)\n",
      "16 18)\n",
      "12 1318)\n",
      "16 1813)\n",
      "1 2, 18)\n",
      "16 18)\n",
      "12 1318)\n",
      "16 1813)\n",
      "1 2, 18)\n",
      "10 12)\n",
      "6 7, 12)\n",
      "10 12)\n",
      "1 2, 12)\n",
      "10 12)\n",
      "6 7, 12)\n",
      "10 12)\n",
      "3 4, 12)\n",
      "9 114)\n",
      "5 6 11)\n",
      "9 116)\n",
      "3 4 11)\n",
      "9 114)\n",
      "5 6 11)\n",
      "9 116)\n",
      "1 2 11)\n",
      "14 16)\n",
      "9 10 16)\n",
      "14 160)\n",
      "1 2, 16)\n",
      "14 16)\n",
      "9 10 16)\n",
      "14 160)\n",
      "1 2, 16)\n",
      "9 112)\n",
      "5 7 11)\n",
      "9 117)\n",
      "1 2 11)\n",
      "9 112)\n",
      "5 7 11)\n",
      "9 117)\n",
      "10 111)\n",
      "18 1911)\n",
      "13 1519)\n",
      "18 1915)\n",
      "10 1119)\n",
      "18 1911)\n",
      "13 1519)\n",
      "18 1915)\n",
      "1 5, 19)\n",
      "12 13)\n",
      "8 9, 13)\n",
      "12 13)\n",
      "1 5, 13)\n",
      "12 13)\n",
      "8 9, 13)\n",
      "12 13)\n",
      "1 3, 13)\n",
      "13 15)\n",
      "7 10 15)\n",
      "13 150)\n",
      "1 3, 15)\n",
      "13 15)\n",
      "7 10 15)\n",
      "13 150)\n",
      "1 2, 15)\n",
      "13 15)\n",
      "8 10 15)\n",
      "13 150)\n",
      "1 2, 15)\n",
      "13 15)\n",
      "8 10 15)\n",
      "13 150)\n",
      "1 3, 15)\n",
      "11 14)\n",
      "7 8, 14)\n",
      "11 14)\n",
      "1 3, 14)\n",
      "11 14)\n",
      "7 8, 14)\n",
      "11 14)\n",
      "1 2, 14)\n",
      "(1, 2)\r"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'start' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [83], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m e1_mask, e2_mask \u001b[39m=\u001b[39m tokenizer(sample[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mspan1_text\u001b[39m\u001b[39m\"\u001b[39m], add_special_tokens\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m], \\\n\u001b[1;32m     10\u001b[0m     tokenizer(sample[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mspan2_text\u001b[39m\u001b[39m\"\u001b[39m], add_special_tokens\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m] \n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(find_sub_list(e1_mask, result[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]), end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[39mprint\u001b[39m(find_sub_list(e2_mask, result[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]), end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [76], line 11\u001b[0m, in \u001b[0;36mfind_sub_list\u001b[0;34m(sublist, main_list)\u001b[0m\n\u001b[1;32m      9\u001b[0m         start \u001b[39m=\u001b[39m idx\n\u001b[1;32m     10\u001b[0m         end \u001b[39m=\u001b[39m idx \u001b[39m+\u001b[39m sublist_length \n\u001b[0;32m---> 11\u001b[0m \u001b[39mprint\u001b[39m(start, end)\n\u001b[1;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m start, end\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'start' referenced before assignment"
     ]
    }
   ],
   "source": [
    "for sample in dataset[\"train\"]:\n",
    "    result = tokenizer(\n",
    "        sample[\"text\"],\n",
    "        truncation=\"longest_first\",\n",
    "        return_token_type_ids=True,\n",
    "        max_length=245,\n",
    "        )\n",
    "\n",
    "    e1_mask, e2_mask = tokenizer(sample[\"target\"][\"span1_text\"], add_special_tokens=False)[\"input_ids\"], \\\n",
    "        tokenizer(sample[\"target\"][\"span2_text\"], add_special_tokens=False)[\"input_ids\"] \n",
    "    print(find_sub_list(e1_mask, result[\"input_ids\"]), end='\\r')\n",
    "    print(find_sub_list(e2_mask, result[\"input_ids\"]), end='\\r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 64,\n",
       " 'target': {'span1_text': '',\n",
       "  'span2_text': ' ',\n",
       "  'span1_index': 0,\n",
       "  'span2_index': 10},\n",
       " 'label': True,\n",
       " 'text': '      ,  ,       .'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def find_sub_list(sublist: List[str], main_list: List[str]):\n",
    "    sublist_length = len(sublist)\n",
    "    for idx, word in enumerate(main_list):\n",
    "        if word == sublist[0] and main_list[idx:idx+sublist_length] == sublist:\n",
    "            start = idx\n",
    "            end = idx + sublist_length\n",
    "            return start, end\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "def process_data(examples, tokenizer: PreTrainedTokenizer, max_length: int):\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=\"longest_first\",\n",
    "        return_token_type_ids=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    e1_mask = np.zeros_like(result[\"input_ids\"], dtype=int)\n",
    "    e2_mask = np.zeros_like(result[\"input_ids\"], dtype=int)\n",
    "\n",
    "    e1_span = sample[\"target\"][\"span1_text\"]\n",
    "    e2_span = sample[\"target\"][\"span2_text\"]\n",
    "\n",
    "    # Find the start and end indices of the spans in the input text\n",
    "    e1_start, e1_end = find_sub_list(e1_span.split(), examples[\"text\"].split())\n",
    "    e2_start, e2_end = find_sub_list(e2_span.split(), examples[\"text\"].split())\n",
    "\n",
    "        # If the spans are not found, just use the indices of the span words\n",
    "    if e1_start is None:\n",
    "        e1_start, e1_end = [i for i, x in enumerate(examples[\"text\"].split()) if x in e1_span.split()], [i+1 for i, x in enumerate(examples[\"text\"].split()) if x in e1_span.split()]\n",
    "    if e2_start is None:\n",
    "        e2_start, e2_end = [i for i, x in enumerate(examples[\"text\"].split()) if x in e2_span.split()], [i+1 for i, x in enumerate(examples[\"text\"].split()) if x in e2_span.split()]\n",
    "    print(e1_start, e1_end)\n",
    "    print(e2_start, e2_end)\n",
    "    # Set the corresponding mask values to 1 for each span\n",
    "    e1_mask[e1_start:e1_end] = 1\n",
    "    e2_mask[e2_start:e2_end] = 1\n",
    "\n",
    "\n",
    "    result[\"e1_mask\"] = e1_mask\n",
    "    result[\"e2_mask\"] = e2_mask\n",
    "\n",
    "    if isinstance(examples[\"label\"], list):\n",
    "        result[\"labels\"] = [int(x) for x in examples[\"label\"]]\n",
    "    else:\n",
    "        result[\"labels\"] = int(examples[\"label\"])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('cointegrated/rubert-tiny2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {'idx': 64,\n",
    " 'target': {'span1_text': '',\n",
    "  'span2_text': ' ',\n",
    "  'span1_index': 0,\n",
    "  'span2_index': 10},\n",
    " 'label': True,\n",
    " 'text': '      ,  ,       .'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "[10, 13] [11, 14]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m process_data(sample, tokenizer, \u001b[39m255\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [31], line 33\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(examples, tokenizer, max_length)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m# Set the corresponding mask values to 1 for each span\u001b[39;00m\n\u001b[1;32m     32\u001b[0m e1_mask[e1_start:e1_end] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 33\u001b[0m e2_mask[e2_start:e2_end] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     36\u001b[0m result[\u001b[39m\"\u001b[39m\u001b[39me1_mask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m e1_mask\n\u001b[1;32m     37\u001b[0m result[\u001b[39m\"\u001b[39m\u001b[39me2_mask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m e2_mask\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "process_data(sample, tokenizer, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39131c8ef7df3c93eb396f46fcd465d060882e79e204152730679d38084e79e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
